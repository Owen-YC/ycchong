import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib.parse
import time
import random
from datetime import datetime
import pandas as pd

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .news-card {
        background-color: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 10px;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .news-title {
        font-size: 1.2rem;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 0.5rem;
    }
    .news-meta {
        font-size: 0.9rem;
        color: #6c757d;
        margin-bottom: 0.5rem;
    }
    .news-description {
        font-size: 1rem;
        color: #495057;
        line-height: 1.5;
    }
    .stButton > button {
        background-color: #1f77b4;
        color: white;
        border-radius: 20px;
        padding: 0.5rem 2rem;
        font-weight: bold;
    }
    .stButton > button:hover {
        background-color: #0056b3;
    }
</style>
""", unsafe_allow_html=True)

def get_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
    ]
    return random.choice(user_agents)

def crawl_google_news(query, num_results=10):
    """êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§"""
    try:
        # êµ¬ê¸€ ë‰´ìŠ¤ URL ìƒì„±
        encoded_query = urllib.parse.quote(query)
        url = f"https://news.google.com/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR%3Ako"
        
        headers = {
            'User-Agent': get_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        time.sleep(random.uniform(1, 2))
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        
        # êµ¬ê¸€ ë‰´ìŠ¤ì˜ ê¸°ì‚¬ ìš”ì†Œë“¤ì„ ì°¾ì•„ì„œ íŒŒì‹±
        news_items = soup.find_all('article', limit=num_results)
        
        for item in news_items:
            try:
                # ì œëª© ì¶”ì¶œ
                title_element = item.find('h3') or item.find('h4')
                if not title_element:
                    continue
                title = title_element.get_text(strip=True)
                
                # URL ì¶”ì¶œ
                link_element = item.find('a')
                if not link_element or not link_element.get('href'):
                    continue
                
                # êµ¬ê¸€ ë‰´ìŠ¤ì˜ ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                article_url = link_element['href']
                if article_url.startswith('./'):
                    article_url = 'https://news.google.com' + article_url[1:]
                elif article_url.startswith('/'):
                    article_url = 'https://news.google.com' + article_url
                
                # ì¶œì²˜ ì¶”ì¶œ
                source_element = item.find('div', {'data-n-tid': True}) or item.find('span')
                source = source_element.get_text(strip=True) if source_element else "Unknown"
                
                # ë°œí–‰ ì‹œê°„ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
                time_element = item.find('time')
                published_time = time_element.get('datetime') if time_element else None
                
                # ì„¤ëª… ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
                description_element = item.find('p') or item.find('span', class_='')
                description = description_element.get_text(strip=True) if description_element else None
                
                if title and article_url:
                    article = {
                        'title': title,
                        'url': article_url,
                        'source': source,
                        'published_time': published_time,
                        'description': description
                    }
                    articles.append(article)
                    
            except Exception as e:
                st.error(f"ê°œë³„ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        return articles
        
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []

def save_to_text(articles, filename):
    """ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ ê²°ê³¼ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
            
            for i, article in enumerate(articles, 1):
                f.write(f"[{i}] {article['title']}\n")
                f.write(f"ì¶œì²˜: {article['source']}\n")
                if article['published_time']:
                    f.write(f"ë°œí–‰ì‹œê°„: {article['published_time']}\n")
                f.write(f"URL: {article['url']}\n")
                if article['description']:
                    f.write(f"ì„¤ëª…: {article['description']}\n")
                f.write("-" * 60 + "\n\n")
        
        return True
    except Exception as e:
        st.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
        return False

def main():
    # í—¤ë”
    st.markdown('<h1 class="main-header">ğŸ“° êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬</h1>', unsafe_allow_html=True)
    st.markdown("### ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¥¼ ì‰½ê³  ë¹ ë¥´ê²Œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ”§ ì„¤ì •")
        
        # ê²€ìƒ‰ ì˜µì…˜
        search_option = st.selectbox(
            "ê²€ìƒ‰ ë°©ë²• ì„ íƒ",
            ["í‚¤ì›Œë“œ ê²€ìƒ‰", "ì¸ê¸° ë‰´ìŠ¤", "ë¹ ë¥¸ ê²€ìƒ‰"]
        )
        
        if search_option == "í‚¤ì›Œë“œ ê²€ìƒ‰":
            query = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: AI, ì½”ë¡œë‚˜, ê²½ì œ...")
            num_results = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 5, 50, 10)
        elif search_option == "ë¹ ë¥¸ ê²€ìƒ‰":
            quick_queries = ["AI", "ì½”ë¡œë‚˜", "ê²½ì œ", "ì •ì¹˜", "ìŠ¤í¬ì¸ ", "IT", "ì—”í„°í…Œì¸ë¨¼íŠ¸"]
            query = st.selectbox("ë¹ ë¥¸ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„ íƒ", quick_queries)
            num_results = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 5, 50, 10)
        else:  # ì¸ê¸° ë‰´ìŠ¤
            query = "í•œêµ­"
            num_results = 15
        
        # ê²€ìƒ‰ ë²„íŠ¼
        if st.button("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰", type="primary"):
            if search_option == "í‚¤ì›Œë“œ ê²€ìƒ‰" and not query.strip():
                st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
                return
            
            with st.spinner("ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
                articles = crawl_google_news(query, num_results)
                
                if articles:
                    st.success(f"âœ… {len(articles)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    
                    # íŒŒì¼ ì €ì¥ ì˜µì…˜
                    if st.button("ğŸ’¾ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"):
                        filename = f"google_news_{query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                        if save_to_text(articles, filename):
                            st.success(f"âœ… ë‰´ìŠ¤ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            
                            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                            with open(filename, 'r', encoding='utf-8') as f:
                                st.download_button(
                                    label="ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                                    data=f.read(),
                                    file_name=filename,
                                    mime="text/plain"
                                )
                else:
                    st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if 'articles' in st.session_state and st.session_state.articles:
        articles = st.session_state.articles
        
        # í†µê³„ ì •ë³´
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", len(articles))
        with col2:
            sources = set(article['source'] for article in articles)
            st.metric("ë‰´ìŠ¤ ì¶œì²˜ ìˆ˜", len(sources))
        with col3:
            st.metric("ê²€ìƒ‰ ì‹œê°„", datetime.now().strftime("%H:%M:%S"))
        
        # ë‰´ìŠ¤ ëª©ë¡
        st.subheader("ğŸ“° ë‰´ìŠ¤ ëª©ë¡")
        
        for i, article in enumerate(articles, 1):
            with st.container():
                st.markdown(f"""
                <div class="news-card">
                    <div class="news-title">{i}. {article['title']}</div>
                    <div class="news-meta">
                        ğŸ“° {article['source']} 
                        {f"| ğŸ•’ {article['published_time']}" if article['published_time'] else ""}
                    </div>
                    <div class="news-description">
                        {article['description'] if article['description'] else "ì„¤ëª… ì—†ìŒ"}
                    </div>
                    <a href="{article['url']}" target="_blank" style="color: #1f77b4; text-decoration: none;">
                        ğŸ”— ì›ë¬¸ ë³´ê¸°
                    </a>
                </div>
                """, unsafe_allow_html=True)
        
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œ
        st.subheader("ğŸ“Š ë°ì´í„° í…Œì´ë¸”")
        df = pd.DataFrame(articles)
        st.dataframe(df, use_container_width=True)
        
        # CSV ë‹¤ìš´ë¡œë“œ
        csv = df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name=f"google_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

if __name__ == "__main__":
    main()
